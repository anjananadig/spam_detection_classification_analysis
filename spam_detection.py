# -*- coding: utf-8 -*-
"""Spam_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vi7DUWLzlcmIpaygRgD_L2CmHSTlPah9
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv('/content/spam.csv', encoding='ISO-8859-1')
print(data)

# from google.colab import files
# uploaded = files.upload()

print(data.head())

data = data.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'])

print(data.head())

data = data.rename(columns={'v1': 'Category', 'v2': 'Message'})

print(data['Category'].value_counts())

data['Category'] = data['Category'].map({'ham': 0, 'spam': 1})

print(data.info())

print(data.isnull().sum())

data.head()

data.tail()

data.columns

data = data.drop_duplicates()
print(f"After removing duplicates, dataset has {len(data)} rows.")
data.head()

import re        #Regular Expression

data['Message'] = data['Message'].apply(lambda x: re.sub(r'\W+', ' ', x.lower()))            #substituting all special charcters and converting into lowercase and applying to the df

print(data.head(11))

data.describe(include="all")

import matplotlib.pyplot as plt

label_counts = data['Category'].value_counts()

plt.bar(['Ham', 'Spam'], label_counts, color=['lightblue', 'grey'])
plt.title('Spam vs Ham Distribution')
plt.xlabel('Label (Ham vs Spam)')
plt.ylabel('Count')
plt.show()

import matplotlib.pyplot as plt

data['word_count'] = data['Message'].apply(lambda x: len(x.split()))

word_count_by_category = data.groupby('Category')['word_count'].mean()

plt.bar(['Ham', 'Spam'], word_count_by_category.values, color='lightblue')
plt.xlabel('Category')
plt.ylabel('Average Word Count')
plt.title('Average Word Count Distribution by Category')
plt.show()

#Split Data
X = data['Message']
y = data['Category']

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(max_features=5000)  # Limiting features to 5000 most important words
X = tfidf.fit_transform(X).toarray()  # Converting text to numerical data

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

from sklearn.naive_bayes import MultinomialNB
model = MultinomialNB()               #Multinomial Naive Bayes model
model.fit(X_train, y_train)            #Fitting the model

y_pred = model.predict(X_test)            #Predictions

#Check Accuracy:

from sklearn.metrics import accuracy_score, confusion_matrix
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

#Confusion Matrix:
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='coolwarm')
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

